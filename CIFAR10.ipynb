{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F  \n",
    "import torchvision.datasets as datasets  \n",
    "import torchvision.transforms as transforms  \n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "experiment = \"CIFAR10\"\n",
    "# Load Data\n",
    "train_set = torchvision.datasets.CIFAR10(root=\"./CIFAR10\", train = True, download = True, transform = data_transforms[\"train\"])\n",
    "test_set = torchvision.datasets.CIFAR10(root=\"./CIFAR10\", train = False, download = True, transform = data_transforms[\"test\"])\n",
    "train_size = len(train_set)\n",
    "test_size = len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 10\n",
      "Number of batches: 6250\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "num_workers = 2\n",
    "classes = train_set.classes\n",
    "num_class = len(classes)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "num_batches = math.floor(train_size/batch_size)\n",
    "print(f\"Number of classes: {num_class}\")\n",
    "print(f\"Number of batches: {num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\dongt/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\n",
      "C:\\Users\\dongt/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\\PyTorch\\Classification\\ConvNets\\image_classification\\models\\common.py:14: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  \"pytorch_quantization module not found, quantization will not be available\"\n",
      "C:\\Users\\dongt/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\\PyTorch\\Classification\\ConvNets\\image_classification\\models\\efficientnet.py:18: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  \"pytorch_quantization module not found, quantization will not be available\"\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_widese_b0', pretrained=True)\n",
    "in_fc = model.classifier.fc.in_features\n",
    "model.classifier.fc = nn.Linear(in_fc, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ada188e3aa04959b73f843a4f5b6e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?epochs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3613a4680da74145bef76b20d1402a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/6250 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4ac635e31a4fcbbee75e587f4371b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16016\\2140300767.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# TensorBoard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_histogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Classifier layer parameter\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training Accuracy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\tensorboard\\writer.py\u001b[0m in \u001b[0;36madd_histogram\u001b[1;34m(self, tag, values, global_step, bins, walltime, max_bins)\u001b[0m\n\u001b[0;32m    439\u001b[0m             \u001b[0mbins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_bins\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m         self._get_file_writer().add_summary(\n\u001b[1;32m--> 441\u001b[1;33m             histogram(tag, values, bins, max_bins=max_bins), global_step, walltime)\n\u001b[0m\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m     def add_histogram_raw(self, tag, min, max, num, sum, sum_squares,\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\tensorboard\\summary.py\u001b[0m in \u001b[0;36mhistogram\u001b[1;34m(name, values, bins, max_bins)\u001b[0m\n\u001b[0;32m    317\u001b[0m       \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \"\"\"\n\u001b[1;32m--> 319\u001b[1;33m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_np\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m     \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_histogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_bins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mSummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhisto\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\tensorboard\\_convert_np.py\u001b[0m in \u001b[0;36mmake_np\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_prepare_pytorch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     raise NotImplementedError(\n\u001b[0;32m     25\u001b[0m         'Got {}, but numpy array, torch tensor, or caffe2 blob name are expected.'.format(type(x)))\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\tensorboard\\_convert_np.py\u001b[0m in \u001b[0;36m_prepare_pytorch\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_prepare_pytorch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "learning_rate = 0.01\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Initialize network\n",
    "model.to(device)\n",
    "model.train()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "writer = SummaryWriter(f\"runs/{experiment}/BatchSize {batch_size} LR {learning_rate}\")\n",
    "\n",
    "# Inspect the model\n",
    "images, _ = next(iter(train_loader))\n",
    "writer.add_graph(model, images.to(device))\n",
    "writer.close()\n",
    "\n",
    "# Progress bar\n",
    "outer = tqdm(total=num_epochs, unit=\"epochs\", position=0, desc=\"Epoch\")\n",
    "inner = tqdm(total=num_batches, unit=\"batches\", position=1, desc=\"Batch\")\n",
    "batch_log = tqdm(total=0, position=3, bar_format='{desc}')\n",
    "\n",
    "# Train\n",
    "epoch_dict = {}\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)                          # N, C, W, H\n",
    "        labels = labels.to(device)                          # N\n",
    "\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, predictions = outputs.max(1)                     # N\n",
    "        step_accuracy = (predictions == labels).sum() / inputs.shape[0]\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        \n",
    "        # update dict at end epoch\n",
    "        if batch_idx == num_batches - 1:\n",
    "            epoch_dict[epoch] = {\"loss\": round(loss.item(), 4),\n",
    "                                 \"accuracy\": round(step_accuracy.item(), 4)}\n",
    "            \n",
    "        # TensorBoard\n",
    "        writer.add_histogram(\"Classifier layer parameter\", model.classifier.fc.weight)\n",
    "        writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "        writer.add_scalar(\"Training Accuracy\", step_accuracy, global_step=step)\n",
    "        \n",
    "        # update progression bar\n",
    "        batch_log.set_description_str(f\"loss: {round(loss.item(), 4)}, accuracy: {step_accuracy}\")\n",
    "        inner.update(1)\n",
    "    outer.update(1)\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in epoch_dict:\n",
    "    print(f\"Epoch {i + 1} - Loss:\", epoch_dict[i][\"loss\"], \"accuracy:\", epoch_dict[i][\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "params_path = \"./model/model_params\"\n",
    "torch.save(model.state_dict(), params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for validation\n",
    "model.load_state_dict(torch.load(params_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be203ce0b3afc4f5c37fbac412025d7ed1d67cabe9dd00b1fc8774c6d6d19d70"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
